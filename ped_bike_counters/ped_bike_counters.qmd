---
title: PLAN 372 Exercise 2 - Data cleaning
---


In this exercise, we'll be working with data from automated bicycle counters in North Carolina,
from the North Carolina Non-Motorized Volume Data Portal (https://itre.ncsu.edu/focus/bike-ped/nc-nmvdp/)
We have hourly data from December 2021-July 2022, on a large number of bicycle paths and lanes throughout North Carolina.

This dataset is messier than the SFpark data; we'll have to do some cleaning in order
to use it.

## Load libraries


```{r}
library(tidyverse)
library(lubridate)
library(ggthemes)

```

## Read the data, and look at the first few rows

```{r}
data = read_csv(here::here("ped_bike_counters", "data", "ncnmvdp.csv"))
head(data)

```

hmm, that looks weird - where are all the column names? It looks like the first two
rows of the file have the date of the extract. With read_csv, we can skip the first
two rows, and start with the third that has the column names.


```{r}
data = read_csv("ped_bike_counters/data/ncnmvdp.csv", skip=2)
head(data)

```

First, let's graph monthly bike and pedestrian numbers for the American Tobacco Trail in Downtown Durham and crossing I-40. We need to figure out where the data for these are. We know each sensor is in a column, let's print out the column names to see which column the data might be in.


```{r}
names(data)

```

These column names are going to be a pain to work with, because they contain spaces and other special characters. We can rename them easily.


```{r}
data = data |>
  rename(
    att_downtown_bike="ATT Downtown, Bicycles (Q2) ATT Downtown, Bicycles",
    att_downtown_ped="ATT Downtown, Pedestrians (Q2) ATT Downtown, Pedestrians",
    att_i40_bike="ATT I-40 Bridge, Bicycles (Q2) ATT I-40 Bridge, Bicycles",
    att_i40_ped="ATT I-40 Bridge, Pedestrians (Q2) ATT I-40 Bridge, Pedestrians"
  ) |>
  # and now we will select only the Time and American Tobacco Trail columns
  select(Time, starts_with("att_"))

```

We can look again at what the data look like now.


```{r}
data

```

Unlike the SFpark data, different trails are in different columns here. While we can work with the data in this "wide" format, many functions in R are designed to work with data in a "long" formatâ€”where each sensor-day is in a separate row. It is very easy to convert the data to this format using the `pivot_longer` function.

This will "pivot" all columns that start with "att" into rows. The "sensor" column will contain the sensor name (which was previously the column name), and the count column will contain the count at that time.


```{r}
data = data |>
  pivot_longer(starts_with("att"), names_to="sensor", values_to="count")

```


The data currently have hourly records. We want a monthly plot.

First, we'll create a column with the month that we can group by. To do that, we parse
the date, then use floor_date to get the beginning of the month.

`lubridate` has date parsing functions named things like `mdy_hms` for
month/day/year, hours:minutes:seconds format. Inspect the Time column and figure
out which function to use here.


```{r}
# answer:

```

Use the floor_date function to get the start of each month.


```{r}
# answer:

```


Next, we can summarize the data by month and sensor, and sum up the count. Assign it to a new variable `monthly_bike_ped`


```{r}
# answer:

```

Now, plot the data, using different colors for different sensors.


```{r}
# answer:

```

## Addressing data gaps?

There are some gaps in our chart - why might this be?

These data are collected by sensors out in the world, which can break, run out of battery, etc. In R, missing data are usually represented by an NA value (some datasets may use 0, -1, -999, etc to signify missing data). When we do math with an NA value, the result is NA - to avoid you accidentally using data that you thought was not missing but is. So if any hourly period is missing during the entire month, the entire month count will be considered NA. R has an is.na function to check for NAs.

Let's count how many observations are NA for each sensor.


```{r}
data |>
  group_by(sensor) |>
  summarize(na_count=sum(is.na(count))) |>
  ungroup()

```


### When are these missing data points? We can use a histogram to find out

Here we will introduce a new ggplot function, `facet_wrap`, which divides your data based on some variable and makes separate small plots for each sub-dataset. Variable names in `facet_wrap` are preceded by a `~`.


```{r}
data |>
  group_by(month_year, sensor) |>
  summarize(proportion_missing = mean(is.na(count))) |>
  ungroup() |>
  ggplot(aes(x=month_year, y=proportion_missing)) +
    geom_col() +
    facet_wrap(~sensor)

```

### What should we do?

In the case of the I-40 sensor, the missing data is a small amount of data, so we might reasonably just replace these records with zeros. This will result in the readings being a little lower during the March 2022, but we can live with that for now. In other cases, various "imputation" techniques can be used to try to guess what the missing data might have been. Getting into these is beyond the scope of this exercise. We'll just apply the same technique to the downtown sensor, and understand that lower demand from pedestrians downtown during December 2021 may be due to data quality issues.

We can remove missing data with the replace_na function from tidyverse. Here, I am putting the data into a new variable count_complete. We could also give it the same nameif we wanted to overwrite the original data with the data where the NA values are replaced with 0. This would overwrite the data in memory, but does not change the file on your disk-so if you ran the read_csv line above again, you would have the original dataset.

We could also do this by filtering to just non-NA values before grouping by sensor and month/year. In this case this would produce the same result, however in other cases it might notâ€”because filtering deletes entire rows of data, whereas replacing NAs only affects that column.


```{r}
data = data |>
  mutate(count_complete=replace_na(count, 0))

```

### Exercise: Plot the data again. Are the gaps gone?


```{r}
# answer:

```

## Which part of the trail is more popular?

Our graphs so far separate out bicycles and pedestrians. Let's add together bicycles and pedestrians. To do this, first use the `case_match` function to create a new `location` column that contains either "Downtown" or "I-40"


```{r}
# answer:

```

Next, make a graph of the total volumes downtown and at I-40. Make sure to handle the missing values!


```{r}
# answer:

```


## Plotting: other geometries

So far, we have used only geom_line and geom_col. But there are many other geom_ options. For instance, geom_col makes a bar plot, geom_histogram a histogram, and geom_boxplot a boxplot.

### Let's make a bar plot by time of day, for each sensor


```{r}
data$hour = hour(data$Time)
hour_totals = data |>
  group_by(hour, location) |>
  summarize(hourly_count = mean(count)) |>
  ungroup()

ggplot(hour_totals, aes(x=hour, y=hourly_count)) +
  geom_col() +
    facet_wrap(~location)

```

Well, that didn't work. Why? Look at the warning message from the ggplot command,
and the hour_totals dataset.

We can remove the missing values by adding na.rm=T to the mean calculation.
You can add na.rm=T to most R function calls to tell it to ignore NA values


```{r}
data$hour = hour(data$Time)
hour_totals = data |>
  group_by(hour, location) |>
  summarize(hourly_count = mean(count, na.rm=T)) |>
  ungroup()

ggplot(hour_totals, aes(x=hour, y=hourly_count)) +
  geom_col() +
    facet_wrap(~location)

```

Alternately, we could filter out the NA values and get the same result.


```{r}
data$hour = hour(data$Time)
hour_totals = data |>
  # retain only rows where count is not NA (! means not)
  filter(!is.na(count)) |>
  group_by(hour, location) |>
  summarize(hourly_count = mean(count)) |>
  ungroup()

ggplot(hour_totals, aes(x=hour, y=hourly_count)) +
  geom_col() +
    facet_wrap(~location)

```

We could also use the count_complete variable instead of removing NAs. Will we get the same answer if we do this? Why or why not?


```{r}
data$hour = hour(data$Time)
hour_totals = data |>
  # retain only rows where count is not NA (! means not)
  group_by(hour, location) |>
  summarize(hourly_count = mean(count_complete)) |>
  ungroup()

ggplot(hour_totals, aes(x=hour, y=hourly_count)) +
  geom_col() +
  facet_wrap(~location)

```


What are the units of the y axis?

The units are the average number of bikes _or_ pedestrians that passed the sensor in that hour. To get the average number of bikes and pedestrians, we need to first sum up the bike and pedestrian counts within each hour, _then_ take the mean.


```{r}
data$hour = hour(data$Time)
hour_totals = data |>
  # retain only rows where count is not NA (! means not)
  filter(!is.na(count)) |>

  # First, sum up within each hour at each location (put bikes and peds together)
  # In this dataset, the Time column contains only whole hours (i.e. there are no records for, say, 8:30)
  # If that were not the case, we'd need to make a new date_hour variable with floor_date.
  group_by(Time, location) |>
  summarize(count=sum(count)) |>

  # Next, take the mean of the hourly totals
  group_by(hour, location) |>
  summarize(hourly_count = mean(count)) |>
  ungroup()

ggplot(hour_totals, aes(x=hour, y=hourly_count)) +
  geom_col() +
    facet_wrap(~location)

```


Why didn't that work?

We lost the hour variable when we did the group_by and summarize. After group_by and summarize, the only variables that remain are (1) the grouping variables, and (2) any variables you created in summarize. The easiest way to solve this is to just add the hour variable to the first summarize. Since the hour is the same within each group when the data are grouped by time and location, we can just use the `first` function to get the hour.


```{r}
data$hour = hour(data$Time)
hour_totals = data |>
  # retain only rows where count is not NA (! means not)
  filter(!is.na(count)) |>

  # First, sum up within each hour at each location (put bikes and peds together)
  # In this dataset, the Time column contains only whole hours (i.e. there are no records for, say, 8:30)
  # If that were not the case, we'd need to make a new date_hour variable with floor_date.
  group_by(Time, location) |>
  summarize(count=sum(count), hour=first(hour)) |>

  # Next, take the mean of the hourly totals
  group_by(hour, location) |>
  summarize(hourly_count = mean(count)) |>
  ungroup()

ggplot(hour_totals, aes(x=hour, y=hourly_count)) +
  geom_col() +
    facet_wrap(~location)

```

Note that the pattern looks similar, but the values on the y axis have changed significantly.

### Exercise: create a plot that shows average activity by day of week (hint: the wday function is helpful)


```{r}
# answer:

```

# Filtering by day and time


Due to weather variation and weekly schedules, there is likely to be a large
spread in the number of trail users by day. These are likely to be very large
seasonally, but even within a shorter time period they are likely to exist.
Let's create a boxplot of daily pedestrian trail usage in January and February
of 2022. There were several major snowstorms (by Triangle standards anyhow) during these
months, so we should see a fairly wide spread.

First, we need to filter our data to just our time period of interest. We can do
this using the filter function we used in the SFpark example, but with multiple
conditions and using inequality rather than equality conditions. We need to create
date objects to compare to, which we do using the ymd_hms function and a literal
string (in double quotes) that will be parsed as a date.


```{r}
jan_feb = data |>
  filter(Time >= ymd_hms("2022-01-01 00:00:00") & Time <= ymd_hms("2022-02-28 23:59:59"))

```

It's always good to look at the data and make sure your filtering worked


```{r}
summary(jan_feb$Time)

```

Now, we need to create a date field so we can get daily totals


```{r}
jan_feb$date = date(jan_feb$Time)

day_totals = jan_feb |>
  filter(!is.na(count)) |>
  group_by(date, location) |>
  summarize(total=sum(count)) |>
  ungroup()

# We put "location" on the x axis so we get two boxplots
ggplot(day_totals, aes(x=location, y=total)) +
  geom_boxplot()

```

### Exercise: repeat the above analysis, looking only at the daytime hours between 7 AM and 5 PM


```{r}
# answer:

```

Do the results change much? Is there a lot of trail use at night?
